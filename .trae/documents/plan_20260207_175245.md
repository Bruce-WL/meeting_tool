# 整体方案反思与优化计划

您指出的问题非常关键。目前系统采用的 **"Map-Reduce"（分片摘要-合并生成）** 架构在处理长文本时，确实存在以下核心痛点，导致了"打地鼠"式的反复故障：

1.  **结构脆弱性（Fragility）**：LLM 在生成长 JSON 时，字段命名经常"漂移"（如 `title` 变 `groupName`），导致严格校验失败，任务直接挂掉。
2.  **上下文丢失（Context Loss）**：在 Reduce 阶段，LLM 需要同时处理摘要、结构化、和创意生成（金句/决策），认知负荷过重，导致靠后的模块（智能章节、金句）容易被忽略或产生幻觉。
3.  **一刀切的验证逻辑**：一旦某个非核心字段不符合 Schema，整个任务就被判定失败，导致用户看到"空白"结果。

为了彻底解决这些问题，我制定了以下**系统性优化方案**：

## 1. 架构升级：引入"模糊归一化"与"容错机制"
不再依赖 LLM "完美"输出，而是增强系统的"兼容能力"。

*   **通用模糊归一化 (Universal Fuzzy Normalizer)**：
    *   实现一个智能适配器，不再硬编码 `if (name) title = name`。
    *   **策略**：递归遍历 JSON，使用同义词词典（`Synonym Dictionary`）自动映射字段。
        *   `['groupName', 'groupTitle', 'name', 'moduleName']` -> `title`
        *   `['content', 'summary', 'description', 'bullets']` -> `points`
    *   **效果**：无论 LLM 输出 `groupName` 还是 `groupTitle`，系统都能自动修正为 `title`，消除 Part 1 校验失败。

*   **柔性验证 (Soft Validation)**：
    *   **策略**：如果严格 Schema 校验失败，先尝试"归一化"修复。如果修复后包含核心字段（如标题、摘要），则**降级通过**，而不是抛出错误。
    *   **效果**：保证用户至少能看到 90% 的内容，而不是因为一个字段错误导致任务全失败。

## 2. 流程优化：强化"智能章节"生成链路
针对"智能章节"等模块经常缺失的问题，优化数据流转：

*   **Map 阶段增强**：在分片摘要时，强制提取 `<Timeline>` 标记，保留时间戳信息。
*   **Prompt 强化**：在 Reduce 阶段，明确要求基于 Map 阶段的时间线数据生成章节，并增加"必须包含"的强指令（已在上一轮部分实施，需配合代码逻辑）。

## 3. 立即修复：数据恢复
针对当前失败的任务 `618b47e4-cafd-417c-9e80-b0dd9116ffce`，我将通过日志重构数据并修复。

---

## 执行步骤

1.  **数据修复**：编写并执行脚本，从日志中提取 `618b47e4` 的有效数据并修复入库。
2.  **代码重构**：
    *   在 `llmService.ts` 中实现 `deepNormalize` 函数（通用模糊归一化）。
    *   修改 `generatePart` 逻辑，在校验失败前优先尝试归一化。
3.  **验证**：提交新任务或使用测试用例，验证系统对"字段漂移"的容错能力。
